{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mMKL runtime not found. Will not attempt to disable multithreaded MKL for parallel rollouts.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import h5py\n",
    "import numpy as np\n",
    "import yaml\n",
    "import os, os.path, shutil\n",
    "\n",
    "from policyopt import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_policy_and_mdp(env_name, policy_state_str):\n",
    "    print(policy_state_str)\n",
    "    import gym\n",
    "    import policyopt\n",
    "    from policyopt import nn, rl\n",
    "    from environments import rlgymenv\n",
    "\n",
    "    # Load the saved state\n",
    "    policy_file, policy_key = util.split_h5_name(policy_state_str)\n",
    "    print 'Loading policy parameters from %s in %s' % (policy_key, policy_file)\n",
    "    with h5py.File(policy_file, 'r') as f:\n",
    "        train_args = json.loads(f.attrs['args'])\n",
    "\n",
    "    # Initialize the MDP\n",
    "    print 'Loading environment', env_name\n",
    "    mdp = rlgymenv.RLGymMDP(env_name)\n",
    "    print 'MDP observation space, action space sizes: %d, %d\\n' % (mdp.obs_space.dim, mdp.action_space.storage_size)\n",
    "\n",
    "    # Initialize the policy\n",
    "    nn.reset_global_scope()\n",
    "    enable_obsnorm = bool(train_args['enable_obsnorm']) if 'enable_obsnorm' in train_args else train_args['obsnorm_mode'] != 'none'\n",
    "    if isinstance(mdp.action_space, policyopt.ContinuousSpace):\n",
    "        if 'sparse_mixture' in policy_file:\n",
    "            policy_cfg = rl.SparseMixtureGaussianPolicyConfig(\n",
    "                hidden_spec=train_args['policy_hidden_spec'],\n",
    "                min_stdev=0.,\n",
    "                init_logstdev=0.,\n",
    "                enable_obsnorm=enable_obsnorm)\n",
    "            policy = rl.SparseMixtureGaussianPolicy(policy_cfg, mdp.obs_space, mdp.action_space, 'GaussianSparseMixturePolicy')\n",
    "        elif 'mixture' in policy_file:\n",
    "            policy_cfg = rl.MixtureGaussianPolicyConfig(\n",
    "                hidden_spec=train_args['policy_hidden_spec'],\n",
    "                min_stdev=0.,\n",
    "                init_logstdev=0.,\n",
    "                enable_obsnorm=enable_obsnorm)\n",
    "            policy = rl.MixtureGaussianPolicy(policy_cfg, mdp.obs_space, mdp.action_space, 'GaussianMixturePolicy')\n",
    "        elif 'ga' in policy_file:\n",
    "            policy_cfg = rl.GaussianPolicyConfig(\n",
    "                hidden_spec=train_args['policy_hidden_spec'],\n",
    "                min_stdev=0.,\n",
    "                init_logstdev=0.,\n",
    "                enable_obsnorm=enable_obsnorm)\n",
    "            policy = rl.GaussianPolicy(policy_cfg, mdp.obs_space, mdp.action_space, 'GaussianPolicy')\n",
    "    else:\n",
    "        if 'gibbs' in policy_file:\n",
    "            policy_cfg = rl.GibbsPolicyConfig(\n",
    "                hidden_spec=train_args['policy_hidden_spec'],\n",
    "                enable_obsnorm=enable_obsnorm)\n",
    "            policy = rl.GibbsPolicy(policy_cfg, mdp.obs_space, mdp.action_space, 'GibbsPolicy')\n",
    "        elif 'sparse' in policy_file:\n",
    "            policy_cfg = rl.SparsePolicyConfig(\n",
    "                hidden_spec=train_args['policy_hidden_spec'],\n",
    "                enable_obsnorm=enable_obsnorm)\n",
    "            policy = rl.SparsePolicy(policy_cfg, mdp.obs_space, mdp.action_space, 'SparsePolicy')\n",
    "        \n",
    "\n",
    "    # Load the policy parameters\n",
    "    policy.load_h5(policy_file, policy_key)\n",
    "\n",
    "    return mdp, policy, train_args\n",
    "\n",
    "\n",
    "def gen_taskname2outfile(spec, assert_not_exists=False):\n",
    "    '''\n",
    "    Generate dataset filenames for each task. Phase 0 (sampling) writes to these files,\n",
    "    phase 1 (training) reads from them.\n",
    "    '''\n",
    "    taskname2outfile = {}\n",
    "    trajdir = os.path.join(spec['options']['storagedir'], spec['options']['traj_subdir'])\n",
    "    util.mkdir_p(trajdir)\n",
    "    for task in spec['tasks']:\n",
    "        assert task['name'] not in taskname2outfile\n",
    "        fname = os.path.join(trajdir, 'trajs_{}.h5'.format(task['name']))\n",
    "        if assert_not_exists:\n",
    "            assert not os.path.exists(fname), 'Traj destination {} already exists'.format(fname)\n",
    "        taskname2outfile[task['name']] = fname\n",
    "    return taskname2outfile\n",
    "\n",
    "\n",
    "\n",
    "def exec_saved_policy(env_name, policystr, num_trajs, deterministic, max_traj_len=None):\n",
    "    import policyopt\n",
    "    from policyopt import SimConfig, rl, util, nn, tqdm\n",
    "    from environments import rlgymenv\n",
    "    import gym\n",
    "\n",
    "    # Load MDP and policy\n",
    "    mdp, policy, _ = load_trained_policy_and_mdp(env_name, policystr)\n",
    "    max_traj_len = min(mdp.env_spec.timestep_limit, max_traj_len) if max_traj_len is not None else mdp.env_spec.timestep_limit\n",
    "\n",
    "    print 'Sampling {} trajs (max len {}) from policy {} in {}'.format(num_trajs, max_traj_len, policystr, env_name)\n",
    "\n",
    "    # Sample trajs\n",
    "    trajbatch = mdp.sim_mp(\n",
    "        policy_fn=lambda obs_B_Do: policy.sample_actions(obs_B_Do, deterministic),\n",
    "        obsfeat_fn=lambda obs:obs,\n",
    "        cfg=policyopt.SimConfig(\n",
    "            min_num_trajs=num_trajs,\n",
    "            min_total_sa=-1,\n",
    "            batch_size=None,\n",
    "            max_traj_len=max_traj_len))\n",
    "\n",
    "    return trajbatch, policy, mdp\n",
    "\n",
    "\n",
    "def eval_snapshot(env_name, checkptfile, snapshot_idx, num_trajs, deterministic):\n",
    "    policystr = '{}/snapshots/iter{:07d}'.format(checkptfile, snapshot_idx)\n",
    "    trajbatch, _, _ = exec_saved_policy(\n",
    "        env_name,\n",
    "        policystr,\n",
    "        num_trajs,\n",
    "        deterministic=deterministic,\n",
    "        max_traj_len=None)\n",
    "    returns = trajbatch.r.padded(fill=0.).sum(axis=1)\n",
    "    lengths = np.array([len(traj) for traj in trajbatch])\n",
    "    util.header('{} gets return {} +/- {}'.format(policystr, returns.mean(), returns.std()))\n",
    "    return returns, lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95m=== Phase 2: evaluating trained models ===\u001b[0m\n",
      "Evaluating results in imitation_runs/test/checkpoints_all\n",
      "Will store results in imitation_runs/test/checkpoints_all/results.h5\n",
      "\u001b[95mEvaluating run 1/12: alg=ga_mixture_0,task=hopper,num_trajs=4,run=0\u001b[0m\n",
      "imitation_runs/test/checkpoints_all/alg=ga_mixture_0,task=hopper,num_trajs=4,run=0.h5/snapshots/iter0000200\n",
      "Loading policy parameters from /snapshots/iter0000200 in imitation_runs/test/checkpoints_all/alg=ga_mixture_0,task=hopper,num_trajs=4,run=0.h5\n",
      "Loading environment Hopper-v1\n",
      "Gym version: 0.9.3\n",
      "MDP observation space, action space sizes: 11, 3\n",
      "\n",
      "\u001b[95mLoading feedforward net specification\u001b[0m\n",
      "[\n",
      "  {\n",
      "    \"type\": \"fc\",\n",
      "    \"n\": 64\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"nonlin\",\n",
      "    \"func\": \"tanh\"\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"fc\",\n",
      "    \"n\": 64\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"nonlin\",\n",
      "    \"func\": \"tanh\"\n",
      "  }\n",
      "]\n",
      "\u001b[95mAffine(in=11, out=64)\u001b[0m\n",
      "\u001b[95mNonlinearity(func=tanh)\u001b[0m\n",
      "\u001b[95mAffine(in=64, out=64)\u001b[0m\n",
      "\u001b[95mNonlinearity(func=tanh)\u001b[0m\n",
      "\u001b[95mAffine(in=64, out=12)\u001b[0m\n",
      "Reading GaussianMixturePolicy/logstdevs_1_Da\n",
      "Reading GaussianMixturePolicy/obsnorm/Standardizer/count\n",
      "Reading GaussianMixturePolicy/obsnorm/Standardizer/mean_1_D\n",
      "Reading GaussianMixturePolicy/obsnorm/Standardizer/meansq_1_D\n",
      "Reading GaussianMixturePolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W\n",
      "Reading GaussianMixturePolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b\n",
      "Reading GaussianMixturePolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W\n",
      "Reading GaussianMixturePolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b\n",
      "Reading GaussianMixturePolicy/out/AffineLayer/W\n",
      "Reading GaussianMixturePolicy/out/AffineLayer/b\n",
      "Sampling 50 trajs (max len 1000) from policy imitation_runs/test/checkpoints_all/alg=ga_mixture_0,task=hopper,num_trajs=4,run=0.h5/snapshots/iter0000200 in Hopper-v1\n",
      "\u001b[95mimitation_runs/test/checkpoints_all/alg=ga_mixture_0,task=hopper,num_trajs=4,run=0.h5/snapshots/iter0000200 gets return 3666.07071019 +/- 4.39528951813\u001b[0m\n",
      "\u001b[95mEvaluating run 2/12: alg=ga_mixture_0,task=hopper,num_trajs=11,run=0\u001b[0m\n",
      "imitation_runs/test/checkpoints_all/alg=ga_mixture_0,task=hopper,num_trajs=11,run=0.h5/snapshots/iter0000200\n",
      "Loading policy parameters from /snapshots/iter0000200 in imitation_runs/test/checkpoints_all/alg=ga_mixture_0,task=hopper,num_trajs=11,run=0.h5\n",
      "Loading environment Hopper-v1\n",
      "Gym version: 0.9.3\n",
      "MDP observation space, action space sizes: 11, 3\n",
      "\n",
      "\u001b[95mLoading feedforward net specification\u001b[0m\n",
      "[\n",
      "  {\n",
      "    \"type\": \"fc\",\n",
      "    \"n\": 64\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"nonlin\",\n",
      "    \"func\": \"tanh\"\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"fc\",\n",
      "    \"n\": 64\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"nonlin\",\n",
      "    \"func\": \"tanh\"\n",
      "  }\n",
      "]\n",
      "\u001b[95mAffine(in=11, out=64)\u001b[0m\n",
      "\u001b[95mNonlinearity(func=tanh)\u001b[0m\n",
      "\u001b[95mAffine(in=64, out=64)\u001b[0m\n",
      "\u001b[95mNonlinearity(func=tanh)\u001b[0m\n",
      "\u001b[95mAffine(in=64, out=12)\u001b[0m\n",
      "Reading GaussianMixturePolicy/logstdevs_1_Da\n",
      "Reading GaussianMixturePolicy/obsnorm/Standardizer/count\n",
      "Reading GaussianMixturePolicy/obsnorm/Standardizer/mean_1_D\n",
      "Reading GaussianMixturePolicy/obsnorm/Standardizer/meansq_1_D\n",
      "Reading GaussianMixturePolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W\n",
      "Reading GaussianMixturePolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b\n",
      "Reading GaussianMixturePolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W\n",
      "Reading GaussianMixturePolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b\n",
      "Reading GaussianMixturePolicy/out/AffineLayer/W\n",
      "Reading GaussianMixturePolicy/out/AffineLayer/b\n",
      "Sampling 50 trajs (max len 1000) from policy imitation_runs/test/checkpoints_all/alg=ga_mixture_0,task=hopper,num_trajs=11,run=0.h5/snapshots/iter0000200 in Hopper-v1\n",
      "\u001b[95mimitation_runs/test/checkpoints_all/alg=ga_mixture_0,task=hopper,num_trajs=11,run=0.h5/snapshots/iter0000200 gets return 3671.33657784 +/- 4.07454228785\u001b[0m\n",
      "\u001b[95mEvaluating run 3/12: alg=ga_mixture_0,task=hopper,num_trajs=18,run=0\u001b[0m\n",
      "imitation_runs/test/checkpoints_all/alg=ga_mixture_0,task=hopper,num_trajs=18,run=0.h5/snapshots/iter0000200\n",
      "Loading policy parameters from /snapshots/iter0000200 in imitation_runs/test/checkpoints_all/alg=ga_mixture_0,task=hopper,num_trajs=18,run=0.h5\n",
      "Loading environment Hopper-v1\n",
      "Gym version: 0.9.3\n",
      "MDP observation space, action space sizes: 11, 3\n",
      "\n",
      "\u001b[95mLoading feedforward net specification\u001b[0m\n",
      "[\n",
      "  {\n",
      "    \"type\": \"fc\",\n",
      "    \"n\": 64\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"nonlin\",\n",
      "    \"func\": \"tanh\"\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"fc\",\n",
      "    \"n\": 64\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"nonlin\",\n",
      "    \"func\": \"tanh\"\n",
      "  }\n",
      "]\n",
      "\u001b[95mAffine(in=11, out=64)\u001b[0m\n",
      "\u001b[95mNonlinearity(func=tanh)\u001b[0m\n",
      "\u001b[95mAffine(in=64, out=64)\u001b[0m\n",
      "\u001b[95mNonlinearity(func=tanh)\u001b[0m\n",
      "\u001b[95mAffine(in=64, out=12)\u001b[0m\n",
      "Reading GaussianMixturePolicy/logstdevs_1_Da\n",
      "Reading GaussianMixturePolicy/obsnorm/Standardizer/count\n",
      "Reading GaussianMixturePolicy/obsnorm/Standardizer/mean_1_D\n",
      "Reading GaussianMixturePolicy/obsnorm/Standardizer/meansq_1_D\n",
      "Reading GaussianMixturePolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W\n",
      "Reading GaussianMixturePolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b\n",
      "Reading GaussianMixturePolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W\n",
      "Reading GaussianMixturePolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b\n",
      "Reading GaussianMixturePolicy/out/AffineLayer/W\n",
      "Reading GaussianMixturePolicy/out/AffineLayer/b\n",
      "Sampling 50 trajs (max len 1000) from policy imitation_runs/test/checkpoints_all/alg=ga_mixture_0,task=hopper,num_trajs=18,run=0.h5/snapshots/iter0000200 in Hopper-v1\n",
      "\u001b[95mimitation_runs/test/checkpoints_all/alg=ga_mixture_0,task=hopper,num_trajs=18,run=0.h5/snapshots/iter0000200 gets return 3603.23979465 +/- 6.79824813436\u001b[0m\n",
      "\u001b[95mEvaluating run 4/12: alg=ga_mixture_0,task=hopper,num_trajs=25,run=0\u001b[0m\n",
      "imitation_runs/test/checkpoints_all/alg=ga_mixture_0,task=hopper,num_trajs=25,run=0.h5/snapshots/iter0000200\n",
      "Loading policy parameters from /snapshots/iter0000200 in imitation_runs/test/checkpoints_all/alg=ga_mixture_0,task=hopper,num_trajs=25,run=0.h5\n",
      "Loading environment Hopper-v1\n",
      "Gym version: 0.9.3\n",
      "MDP observation space, action space sizes: 11, 3\n",
      "\n",
      "\u001b[95mLoading feedforward net specification\u001b[0m\n",
      "[\n",
      "  {\n",
      "    \"type\": \"fc\",\n",
      "    \"n\": 64\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"nonlin\",\n",
      "    \"func\": \"tanh\"\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"fc\",\n",
      "    \"n\": 64\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"nonlin\",\n",
      "    \"func\": \"tanh\"\n",
      "  }\n",
      "]\n",
      "\u001b[95mAffine(in=11, out=64)\u001b[0m\n",
      "\u001b[95mNonlinearity(func=tanh)\u001b[0m\n",
      "\u001b[95mAffine(in=64, out=64)\u001b[0m\n",
      "\u001b[95mNonlinearity(func=tanh)\u001b[0m\n",
      "\u001b[95mAffine(in=64, out=12)\u001b[0m\n",
      "Reading GaussianMixturePolicy/logstdevs_1_Da\n",
      "Reading GaussianMixturePolicy/obsnorm/Standardizer/count\n",
      "Reading GaussianMixturePolicy/obsnorm/Standardizer/mean_1_D\n",
      "Reading GaussianMixturePolicy/obsnorm/Standardizer/meansq_1_D\n",
      "Reading GaussianMixturePolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W\n",
      "Reading GaussianMixturePolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b\n",
      "Reading GaussianMixturePolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W\n",
      "Reading GaussianMixturePolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b\n",
      "Reading GaussianMixturePolicy/out/AffineLayer/W\n",
      "Reading GaussianMixturePolicy/out/AffineLayer/b\n",
      "Sampling 50 trajs (max len 1000) from policy imitation_runs/test/checkpoints_all/alg=ga_mixture_0,task=hopper,num_trajs=25,run=0.h5/snapshots/iter0000200 in Hopper-v1\n",
      "\u001b[95mimitation_runs/test/checkpoints_all/alg=ga_mixture_0,task=hopper,num_trajs=25,run=0.h5/snapshots/iter0000200 gets return 3631.79722087 +/- 3.03773357676\u001b[0m\n",
      "\u001b[95mEvaluating run 5/12: alg=ga_mixture_1,task=hopper,num_trajs=4,run=0\u001b[0m\n",
      "imitation_runs/test/checkpoints_all/alg=ga_mixture_1,task=hopper,num_trajs=4,run=0.h5/snapshots/iter0000200\n",
      "Loading policy parameters from /snapshots/iter0000200 in imitation_runs/test/checkpoints_all/alg=ga_mixture_1,task=hopper,num_trajs=4,run=0.h5\n",
      "Loading environment Hopper-v1\n",
      "Gym version: 0.9.3\n",
      "MDP observation space, action space sizes: 11, 3\n",
      "\n",
      "\u001b[95mLoading feedforward net specification\u001b[0m\n",
      "[\n",
      "  {\n",
      "    \"type\": \"fc\",\n",
      "    \"n\": 64\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"nonlin\",\n",
      "    \"func\": \"tanh\"\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"fc\",\n",
      "    \"n\": 64\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"nonlin\",\n",
      "    \"func\": \"tanh\"\n",
      "  }\n",
      "]\n",
      "\u001b[95mAffine(in=11, out=64)\u001b[0m\n",
      "\u001b[95mNonlinearity(func=tanh)\u001b[0m\n",
      "\u001b[95mAffine(in=64, out=64)\u001b[0m\n",
      "\u001b[95mNonlinearity(func=tanh)\u001b[0m\n",
      "\u001b[95mAffine(in=64, out=12)\u001b[0m\n",
      "Reading GaussianMixturePolicy/logstdevs_1_Da\n",
      "Reading GaussianMixturePolicy/obsnorm/Standardizer/count\n",
      "Reading GaussianMixturePolicy/obsnorm/Standardizer/mean_1_D\n",
      "Reading GaussianMixturePolicy/obsnorm/Standardizer/meansq_1_D\n",
      "Reading GaussianMixturePolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W\n",
      "Reading GaussianMixturePolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b\n",
      "Reading GaussianMixturePolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W\n",
      "Reading GaussianMixturePolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b\n",
      "Reading GaussianMixturePolicy/out/AffineLayer/W\n",
      "Reading GaussianMixturePolicy/out/AffineLayer/b\n",
      "Sampling 50 trajs (max len 1000) from policy imitation_runs/test/checkpoints_all/alg=ga_mixture_1,task=hopper,num_trajs=4,run=0.h5/snapshots/iter0000200 in Hopper-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mimitation_runs/test/checkpoints_all/alg=ga_mixture_1,task=hopper,num_trajs=4,run=0.h5/snapshots/iter0000200 gets return 185.140935122 +/- 111.055037755\u001b[0m\n",
      "\u001b[95mEvaluating run 6/12: alg=ga_mixture_1,task=hopper,num_trajs=11,run=0\u001b[0m\n",
      "imitation_runs/test/checkpoints_all/alg=ga_mixture_1,task=hopper,num_trajs=11,run=0.h5/snapshots/iter0000200\n",
      "Loading policy parameters from /snapshots/iter0000200 in imitation_runs/test/checkpoints_all/alg=ga_mixture_1,task=hopper,num_trajs=11,run=0.h5\n",
      "Loading environment Hopper-v1\n",
      "Gym version: 0.9.3\n",
      "MDP observation space, action space sizes: 11, 3\n",
      "\n",
      "\u001b[95mLoading feedforward net specification\u001b[0m\n",
      "[\n",
      "  {\n",
      "    \"type\": \"fc\",\n",
      "    \"n\": 64\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"nonlin\",\n",
      "    \"func\": \"tanh\"\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"fc\",\n",
      "    \"n\": 64\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"nonlin\",\n",
      "    \"func\": \"tanh\"\n",
      "  }\n",
      "]\n",
      "\u001b[95mAffine(in=11, out=64)\u001b[0m\n",
      "\u001b[95mNonlinearity(func=tanh)\u001b[0m\n",
      "\u001b[95mAffine(in=64, out=64)\u001b[0m\n",
      "\u001b[95mNonlinearity(func=tanh)\u001b[0m\n",
      "\u001b[95mAffine(in=64, out=12)\u001b[0m\n",
      "Reading GaussianMixturePolicy/logstdevs_1_Da\n",
      "Reading GaussianMixturePolicy/obsnorm/Standardizer/count\n",
      "Reading GaussianMixturePolicy/obsnorm/Standardizer/mean_1_D\n",
      "Reading GaussianMixturePolicy/obsnorm/Standardizer/meansq_1_D\n",
      "Reading GaussianMixturePolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W\n",
      "Reading GaussianMixturePolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b\n",
      "Reading GaussianMixturePolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W\n",
      "Reading GaussianMixturePolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b\n",
      "Reading GaussianMixturePolicy/out/AffineLayer/W\n",
      "Reading GaussianMixturePolicy/out/AffineLayer/b\n",
      "Sampling 50 trajs (max len 1000) from policy imitation_runs/test/checkpoints_all/alg=ga_mixture_1,task=hopper,num_trajs=11,run=0.h5/snapshots/iter0000200 in Hopper-v1\n",
      "\u001b[95mimitation_runs/test/checkpoints_all/alg=ga_mixture_1,task=hopper,num_trajs=11,run=0.h5/snapshots/iter0000200 gets return 194.395554206 +/- 125.44341064\u001b[0m\n",
      "\u001b[95mEvaluating run 7/12: alg=ga_mixture_1,task=hopper,num_trajs=18,run=0\u001b[0m\n",
      "imitation_runs/test/checkpoints_all/alg=ga_mixture_1,task=hopper,num_trajs=18,run=0.h5/snapshots/iter0000200\n",
      "Loading policy parameters from /snapshots/iter0000200 in imitation_runs/test/checkpoints_all/alg=ga_mixture_1,task=hopper,num_trajs=18,run=0.h5\n",
      "Loading environment Hopper-v1\n",
      "Gym version: 0.9.3\n",
      "MDP observation space, action space sizes: 11, 3\n",
      "\n",
      "\u001b[95mLoading feedforward net specification\u001b[0m\n",
      "[\n",
      "  {\n",
      "    \"type\": \"fc\",\n",
      "    \"n\": 64\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"nonlin\",\n",
      "    \"func\": \"tanh\"\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"fc\",\n",
      "    \"n\": 64\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"nonlin\",\n",
      "    \"func\": \"tanh\"\n",
      "  }\n",
      "]\n",
      "\u001b[95mAffine(in=11, out=64)\u001b[0m\n",
      "\u001b[95mNonlinearity(func=tanh)\u001b[0m\n",
      "\u001b[95mAffine(in=64, out=64)\u001b[0m\n",
      "\u001b[95mNonlinearity(func=tanh)\u001b[0m\n",
      "\u001b[95mAffine(in=64, out=12)\u001b[0m\n",
      "Reading GaussianMixturePolicy/logstdevs_1_Da\n",
      "Reading GaussianMixturePolicy/obsnorm/Standardizer/count\n",
      "Reading GaussianMixturePolicy/obsnorm/Standardizer/mean_1_D\n",
      "Reading GaussianMixturePolicy/obsnorm/Standardizer/meansq_1_D\n",
      "Reading GaussianMixturePolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W\n",
      "Reading GaussianMixturePolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b\n",
      "Reading GaussianMixturePolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W\n",
      "Reading GaussianMixturePolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b\n",
      "Reading GaussianMixturePolicy/out/AffineLayer/W\n",
      "Reading GaussianMixturePolicy/out/AffineLayer/b\n",
      "Sampling 50 trajs (max len 1000) from policy imitation_runs/test/checkpoints_all/alg=ga_mixture_1,task=hopper,num_trajs=18,run=0.h5/snapshots/iter0000200 in Hopper-v1\n",
      "\u001b[95mimitation_runs/test/checkpoints_all/alg=ga_mixture_1,task=hopper,num_trajs=18,run=0.h5/snapshots/iter0000200 gets return 404.456015696 +/- 118.744390758\u001b[0m\n",
      "\u001b[95mEvaluating run 8/12: alg=ga_mixture_1,task=hopper,num_trajs=25,run=0\u001b[0m\n",
      "imitation_runs/test/checkpoints_all/alg=ga_mixture_1,task=hopper,num_trajs=25,run=0.h5/snapshots/iter0000200\n",
      "Loading policy parameters from /snapshots/iter0000200 in imitation_runs/test/checkpoints_all/alg=ga_mixture_1,task=hopper,num_trajs=25,run=0.h5\n",
      "Loading environment Hopper-v1\n",
      "Gym version: 0.9.3\n",
      "MDP observation space, action space sizes: 11, 3\n",
      "\n",
      "\u001b[95mLoading feedforward net specification\u001b[0m\n",
      "[\n",
      "  {\n",
      "    \"type\": \"fc\",\n",
      "    \"n\": 64\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"nonlin\",\n",
      "    \"func\": \"tanh\"\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"fc\",\n",
      "    \"n\": 64\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"nonlin\",\n",
      "    \"func\": \"tanh\"\n",
      "  }\n",
      "]\n",
      "\u001b[95mAffine(in=11, out=64)\u001b[0m\n",
      "\u001b[95mNonlinearity(func=tanh)\u001b[0m\n",
      "\u001b[95mAffine(in=64, out=64)\u001b[0m\n",
      "\u001b[95mNonlinearity(func=tanh)\u001b[0m\n",
      "\u001b[95mAffine(in=64, out=12)\u001b[0m\n",
      "Reading GaussianMixturePolicy/logstdevs_1_Da\n",
      "Reading GaussianMixturePolicy/obsnorm/Standardizer/count\n",
      "Reading GaussianMixturePolicy/obsnorm/Standardizer/mean_1_D\n",
      "Reading GaussianMixturePolicy/obsnorm/Standardizer/meansq_1_D\n",
      "Reading GaussianMixturePolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W\n",
      "Reading GaussianMixturePolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b\n",
      "Reading GaussianMixturePolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W\n",
      "Reading GaussianMixturePolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b\n",
      "Reading GaussianMixturePolicy/out/AffineLayer/W\n",
      "Reading GaussianMixturePolicy/out/AffineLayer/b\n",
      "Sampling 50 trajs (max len 1000) from policy imitation_runs/test/checkpoints_all/alg=ga_mixture_1,task=hopper,num_trajs=25,run=0.h5/snapshots/iter0000200 in Hopper-v1\n",
      "\u001b[95mimitation_runs/test/checkpoints_all/alg=ga_mixture_1,task=hopper,num_trajs=25,run=0.h5/snapshots/iter0000200 gets return 377.796920653 +/- 233.787236425\u001b[0m\n",
      "\u001b[95mEvaluating run 9/12: alg=ga,task=hopper,num_trajs=4,run=0\u001b[0m\n",
      "imitation_runs/test/checkpoints_all/alg=ga,task=hopper,num_trajs=4,run=0.h5/snapshots/iter0000200\n",
      "Loading policy parameters from /snapshots/iter0000200 in imitation_runs/test/checkpoints_all/alg=ga,task=hopper,num_trajs=4,run=0.h5\n",
      "Loading environment Hopper-v1\n",
      "Gym version: 0.9.3\n",
      "MDP observation space, action space sizes: 11, 3\n",
      "\n",
      "\u001b[95mLoading feedforward net specification\u001b[0m\n",
      "[\n",
      "  {\n",
      "    \"type\": \"fc\",\n",
      "    \"n\": 64\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"nonlin\",\n",
      "    \"func\": \"tanh\"\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"fc\",\n",
      "    \"n\": 64\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"nonlin\",\n",
      "    \"func\": \"tanh\"\n",
      "  }\n",
      "]\n",
      "\u001b[95mAffine(in=11, out=64)\u001b[0m\n",
      "\u001b[95mNonlinearity(func=tanh)\u001b[0m\n",
      "\u001b[95mAffine(in=64, out=64)\u001b[0m\n",
      "\u001b[95mNonlinearity(func=tanh)\u001b[0m\n",
      "\u001b[95mAffine(in=64, out=3)\u001b[0m\n",
      "Reading GaussianPolicy/logstdevs_1_Da\n",
      "Reading GaussianPolicy/obsnorm/Standardizer/count\n",
      "Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D\n",
      "Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D\n",
      "Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W\n",
      "Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b\n",
      "Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W\n",
      "Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b\n",
      "Reading GaussianPolicy/out/AffineLayer/W\n",
      "Reading GaussianPolicy/out/AffineLayer/b\n",
      "Sampling 50 trajs (max len 1000) from policy imitation_runs/test/checkpoints_all/alg=ga,task=hopper,num_trajs=4,run=0.h5/snapshots/iter0000200 in Hopper-v1\n",
      "\u001b[95mimitation_runs/test/checkpoints_all/alg=ga,task=hopper,num_trajs=4,run=0.h5/snapshots/iter0000200 gets return 3621.98965124 +/- 4.04917497447\u001b[0m\n",
      "\u001b[95mEvaluating run 10/12: alg=ga,task=hopper,num_trajs=11,run=0\u001b[0m\n",
      "imitation_runs/test/checkpoints_all/alg=ga,task=hopper,num_trajs=11,run=0.h5/snapshots/iter0000200\n",
      "Loading policy parameters from /snapshots/iter0000200 in imitation_runs/test/checkpoints_all/alg=ga,task=hopper,num_trajs=11,run=0.h5\n",
      "Loading environment Hopper-v1\n",
      "Gym version: 0.9.3\n",
      "MDP observation space, action space sizes: 11, 3\n",
      "\n",
      "\u001b[95mLoading feedforward net specification\u001b[0m\n",
      "[\n",
      "  {\n",
      "    \"type\": \"fc\",\n",
      "    \"n\": 64\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"nonlin\",\n",
      "    \"func\": \"tanh\"\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"fc\",\n",
      "    \"n\": 64\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"nonlin\",\n",
      "    \"func\": \"tanh\"\n",
      "  }\n",
      "]\n",
      "\u001b[95mAffine(in=11, out=64)\u001b[0m\n",
      "\u001b[95mNonlinearity(func=tanh)\u001b[0m\n",
      "\u001b[95mAffine(in=64, out=64)\u001b[0m\n",
      "\u001b[95mNonlinearity(func=tanh)\u001b[0m\n",
      "\u001b[95mAffine(in=64, out=3)\u001b[0m\n",
      "Reading GaussianPolicy/logstdevs_1_Da\n",
      "Reading GaussianPolicy/obsnorm/Standardizer/count\n",
      "Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D\n",
      "Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D\n",
      "Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W\n",
      "Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b\n",
      "Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W\n",
      "Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b\n",
      "Reading GaussianPolicy/out/AffineLayer/W\n",
      "Reading GaussianPolicy/out/AffineLayer/b\n",
      "Sampling 50 trajs (max len 1000) from policy imitation_runs/test/checkpoints_all/alg=ga,task=hopper,num_trajs=11,run=0.h5/snapshots/iter0000200 in Hopper-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mimitation_runs/test/checkpoints_all/alg=ga,task=hopper,num_trajs=11,run=0.h5/snapshots/iter0000200 gets return 3638.76535412 +/- 10.4435862227\u001b[0m\n",
      "\u001b[95mEvaluating run 11/12: alg=ga,task=hopper,num_trajs=18,run=0\u001b[0m\n",
      "imitation_runs/test/checkpoints_all/alg=ga,task=hopper,num_trajs=18,run=0.h5/snapshots/iter0000200\n",
      "Loading policy parameters from /snapshots/iter0000200 in imitation_runs/test/checkpoints_all/alg=ga,task=hopper,num_trajs=18,run=0.h5\n",
      "Loading environment Hopper-v1\n",
      "Gym version: 0.9.3\n",
      "MDP observation space, action space sizes: 11, 3\n",
      "\n",
      "\u001b[95mLoading feedforward net specification\u001b[0m\n",
      "[\n",
      "  {\n",
      "    \"type\": \"fc\",\n",
      "    \"n\": 64\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"nonlin\",\n",
      "    \"func\": \"tanh\"\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"fc\",\n",
      "    \"n\": 64\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"nonlin\",\n",
      "    \"func\": \"tanh\"\n",
      "  }\n",
      "]\n",
      "\u001b[95mAffine(in=11, out=64)\u001b[0m\n",
      "\u001b[95mNonlinearity(func=tanh)\u001b[0m\n",
      "\u001b[95mAffine(in=64, out=64)\u001b[0m\n",
      "\u001b[95mNonlinearity(func=tanh)\u001b[0m\n",
      "\u001b[95mAffine(in=64, out=3)\u001b[0m\n",
      "Reading GaussianPolicy/logstdevs_1_Da\n",
      "Reading GaussianPolicy/obsnorm/Standardizer/count\n",
      "Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D\n",
      "Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D\n",
      "Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W\n",
      "Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b\n",
      "Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W\n",
      "Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b\n",
      "Reading GaussianPolicy/out/AffineLayer/W\n",
      "Reading GaussianPolicy/out/AffineLayer/b\n",
      "Sampling 50 trajs (max len 1000) from policy imitation_runs/test/checkpoints_all/alg=ga,task=hopper,num_trajs=18,run=0.h5/snapshots/iter0000200 in Hopper-v1\n",
      "\u001b[95mimitation_runs/test/checkpoints_all/alg=ga,task=hopper,num_trajs=18,run=0.h5/snapshots/iter0000200 gets return 3613.28361675 +/- 4.37457951151\u001b[0m\n",
      "\u001b[95mEvaluating run 12/12: alg=ga,task=hopper,num_trajs=25,run=0\u001b[0m\n",
      "imitation_runs/test/checkpoints_all/alg=ga,task=hopper,num_trajs=25,run=0.h5/snapshots/iter0000200\n",
      "Loading policy parameters from /snapshots/iter0000200 in imitation_runs/test/checkpoints_all/alg=ga,task=hopper,num_trajs=25,run=0.h5\n",
      "Loading environment Hopper-v1\n",
      "Gym version: 0.9.3\n",
      "MDP observation space, action space sizes: 11, 3\n",
      "\n",
      "\u001b[95mLoading feedforward net specification\u001b[0m\n",
      "[\n",
      "  {\n",
      "    \"type\": \"fc\",\n",
      "    \"n\": 64\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"nonlin\",\n",
      "    \"func\": \"tanh\"\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"fc\",\n",
      "    \"n\": 64\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"nonlin\",\n",
      "    \"func\": \"tanh\"\n",
      "  }\n",
      "]\n",
      "\u001b[95mAffine(in=11, out=64)\u001b[0m\n",
      "\u001b[95mNonlinearity(func=tanh)\u001b[0m\n",
      "\u001b[95mAffine(in=64, out=64)\u001b[0m\n",
      "\u001b[95mNonlinearity(func=tanh)\u001b[0m\n",
      "\u001b[95mAffine(in=64, out=3)\u001b[0m\n",
      "Reading GaussianPolicy/logstdevs_1_Da\n",
      "Reading GaussianPolicy/obsnorm/Standardizer/count\n",
      "Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D\n",
      "Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D\n",
      "Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W\n",
      "Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b\n",
      "Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W\n",
      "Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b\n",
      "Reading GaussianPolicy/out/AffineLayer/W\n",
      "Reading GaussianPolicy/out/AffineLayer/b\n",
      "Sampling 50 trajs (max len 1000) from policy imitation_runs/test/checkpoints_all/alg=ga,task=hopper,num_trajs=25,run=0.h5/snapshots/iter0000200 in Hopper-v1\n",
      "\u001b[95mimitation_runs/test/checkpoints_all/alg=ga,task=hopper,num_trajs=25,run=0.h5/snapshots/iter0000200 gets return 3611.75583947 +/- 2.66515575378\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py:2882: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block1_values] [items->['alg', 'alg_traj_lengths', 'alg_traj_returns', 'ex_traj_lengths', 'ex_traj_returns', 'task']]\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "with open('./pipelines/im_test_gail_pipeline.yaml', 'r') as f:\n",
    "# with open('./pipelines/im_classic_pipeline_kj.yaml', 'r') as f:\n",
    "    spec = yaml.load(f)\n",
    "    \n",
    "util.header('=== Phase 2: evaluating trained models ===')\n",
    "import pandas as pd\n",
    "\n",
    "taskname2dset = gen_taskname2outfile(spec)\n",
    "\n",
    "# This is where model logs are stored.\n",
    "# We will also store the evaluation here.\n",
    "checkptdir = os.path.join(spec['options']['storagedir'], spec['options']['checkpt_subdir'])\n",
    "print 'Evaluating results in {}'.format(checkptdir)\n",
    "\n",
    "results_full_path = os.path.join(checkptdir, spec['options']['results_filename'])\n",
    "print 'Will store results in {}'.format(results_full_path)\n",
    "# if os.path.exists(results_full_path):\n",
    "#     raise RuntimeError('Results file {} already exists'.format(results_full_path))\n",
    "\n",
    "# First, pre-determine which evaluations we have to do\n",
    "evals_to_do = []\n",
    "nonexistent_checkptfiles = []\n",
    "for task in spec['tasks']:\n",
    "    # See how well the algorithms did...\n",
    "    for alg in spec['training']['algorithms']:\n",
    "        # ...on various dataset sizes\n",
    "        for num_trajs in spec['training']['dataset_num_trajs']:\n",
    "            # for each rerun, for mean / error bars later\n",
    "            for run in range(spec['training']['runs']):\n",
    "                # Make sure the checkpoint file exists (maybe PBS dropped some jobs)\n",
    "                strid = 'alg={},task={},num_trajs={},run={}'.format(alg['name'], task['name'], num_trajs, run)\n",
    "                checkptfile = os.path.join(checkptdir, strid + '.h5')\n",
    "                \n",
    "                if not os.path.exists(checkptfile):\n",
    "                    nonexistent_checkptfiles.append(checkptfile)\n",
    "                evals_to_do.append((task, alg, num_trajs, run, checkptfile))\n",
    "\n",
    "if nonexistent_checkptfiles:\n",
    "    print 'Cannot find checkpoint files:\\n', '\\n'.join(nonexistent_checkptfiles)\n",
    "    raise RuntimeError\n",
    "\n",
    "# Walk through all saved checkpoints\n",
    "collected_results = []\n",
    "for i_eval, (task, alg, num_trajs, run, checkptfile) in enumerate(evals_to_do):\n",
    "    util.header('Evaluating run {}/{}: alg={},task={},num_trajs={},run={}'.format(\n",
    "        i_eval+1, len(evals_to_do), alg['name'], task['name'], num_trajs, run))\n",
    "\n",
    "    # Load the task's traj dataset to see how well the expert does\n",
    "    with h5py.File(taskname2dset[task['name']], 'r') as trajf:\n",
    "        # Expert's true return and traj lengths\n",
    "        ex_traj_returns = trajf['r_B_T'][...].sum(axis=1)\n",
    "        ex_traj_lengths = trajf['len_B'][...]\n",
    "\n",
    "    # Load the checkpoint file\n",
    "    with pd.HDFStore(checkptfile, 'r') as f:\n",
    "        log_df = f['log']\n",
    "        log_df.set_index('iter', inplace=True)\n",
    "\n",
    "        # Evaluate true return for the learned policy\n",
    "        if any(alg['name'].startswith(s) for s in ('bclone_gibbs', 'bclone_sparse', 'bclone_mixture', 'bclone_gauss')):\n",
    "            # Pick the policy with the best validation accuracy\n",
    "            best_snapshot_idx = log_df['valacc'].argmax()\n",
    "            alg_traj_returns, alg_traj_lengths = eval_snapshot(\n",
    "                task['env'], checkptfile, best_snapshot_idx,\n",
    "                spec['options']['eval_num_trajs'], deterministic=True)\n",
    "\n",
    "        elif any(alg['name'].startswith(s) for s in ('ga_mixture', 'ga', 'fem', 'simplex')):\n",
    "            # Evaluate the last saved snapshot\n",
    "            snapshot_names = f.root.snapshots._v_children.keys()\n",
    "            assert all(name.startswith('iter') for name in snapshot_names)\n",
    "            snapshot_inds = sorted([int(name[len('iter'):]) for name in snapshot_names])\n",
    "            best_snapshot_idx = snapshot_inds[-1]\n",
    "            alg_traj_returns, alg_traj_lengths = eval_snapshot(\n",
    "                task['env'], checkptfile, best_snapshot_idx,\n",
    "                spec['options']['eval_num_trajs'], deterministic=True)\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Analysis not implemented for {}'.format(alg['name']))\n",
    "\n",
    "        collected_results.append({\n",
    "            # Trial info\n",
    "            'alg': alg['name'],\n",
    "            'task': task['name'],\n",
    "            'num_trajs': num_trajs,\n",
    "            'run': run,\n",
    "            # Expert performance\n",
    "            'ex_traj_returns': ex_traj_returns,\n",
    "            'ex_traj_lengths': ex_traj_lengths,\n",
    "            # Learned policy performance\n",
    "            'alg_traj_returns': alg_traj_returns,\n",
    "            'alg_traj_lengths': alg_traj_lengths,\n",
    "        })\n",
    "\n",
    "collected_results = pd.DataFrame(collected_results)\n",
    "with pd.HDFStore(results_full_path, 'w') as outf:\n",
    "    outf['results'] = collected_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
