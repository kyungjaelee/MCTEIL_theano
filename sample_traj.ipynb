{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mMKL runtime not found. Will not attempt to disable multithreaded MKL for parallel rollouts.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import h5py\n",
    "import numpy as np\n",
    "import yaml\n",
    "import os, os.path, shutil\n",
    "\n",
    "from policyopt import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_policy_and_mdp(env_name, policy_state_str):\n",
    "    import gym\n",
    "    import policyopt\n",
    "    from policyopt import nn, rl\n",
    "    from environments import rlgymenv\n",
    "\n",
    "    # Load the saved state\n",
    "    policy_file, policy_key = util.split_h5_name(policy_state_str)\n",
    "    print 'Loading policy parameters from %s in %s' % (policy_key, policy_file)\n",
    "    with h5py.File(policy_file, 'r') as f:\n",
    "        train_args = json.loads(f.attrs['args'])\n",
    "\n",
    "    # Initialize the MDP\n",
    "    print 'Loading environment', env_name\n",
    "    mdp = rlgymenv.RLGymMDP(env_name)\n",
    "    print 'MDP observation space, action space sizes: %d, %d\\n' % (mdp.obs_space.dim, mdp.action_space.storage_size)\n",
    "\n",
    "    # Initialize the policy\n",
    "    nn.reset_global_scope()\n",
    "    enable_obsnorm = bool(train_args['enable_obsnorm']) if 'enable_obsnorm' in train_args else train_args['obsnorm_mode'] != 'none'\n",
    "    if isinstance(mdp.action_space, policyopt.ContinuousSpace):\n",
    "        policy_cfg = rl.GaussianPolicyConfig(\n",
    "            hidden_spec=train_args['policy_hidden_spec'],\n",
    "            min_stdev=0.,\n",
    "            init_logstdev=0.,\n",
    "            enable_obsnorm=enable_obsnorm)\n",
    "        policy = rl.GaussianPolicy(policy_cfg, mdp.obs_space, mdp.action_space, 'GaussianPolicy')\n",
    "    else:\n",
    "        policy_cfg = rl.GibbsPolicyConfig(\n",
    "            hidden_spec=train_args['policy_hidden_spec'],\n",
    "            enable_obsnorm=enable_obsnorm)\n",
    "        policy = rl.GibbsPolicy(policy_cfg, mdp.obs_space, mdp.action_space, 'GibbsPolicy')\n",
    "\n",
    "    # Load the policy parameters\n",
    "    policy.load_h5(policy_file, policy_key)\n",
    "\n",
    "    return mdp, policy, train_args\n",
    "\n",
    "def gen_taskname2outfile(spec, assert_not_exists=False):\n",
    "    '''\n",
    "    Generate dataset filenames for each task. Phase 0 (sampling) writes to these files,\n",
    "    phase 1 (training) reads from them.\n",
    "    '''\n",
    "    taskname2outfile = {}\n",
    "    trajdir = os.path.join(spec['options']['storagedir'], spec['options']['traj_subdir'])\n",
    "    util.mkdir_p(trajdir)\n",
    "    for task in spec['tasks']:\n",
    "        assert task['name'] not in taskname2outfile\n",
    "        fname = os.path.join(trajdir, 'trajs_{}.h5'.format(task['name']))\n",
    "        # if assert_not_exists:\n",
    "        #     assert not os.path.exists(fname), 'Traj destination {} already exists'.format(fname)\n",
    "        taskname2outfile[task['name']] = fname\n",
    "    return taskname2outfile\n",
    "\n",
    "\n",
    "\n",
    "def exec_saved_policy(env_name, policystr, num_trajs, deterministic, max_traj_len=None):\n",
    "    import policyopt\n",
    "    from policyopt import SimConfig, rl, util, nn, tqdm\n",
    "    from environments import rlgymenv\n",
    "    import gym\n",
    "\n",
    "    # Load MDP and policy\n",
    "    mdp, policy, _ = load_trained_policy_and_mdp(env_name, policystr)\n",
    "    max_traj_len = min(mdp.env_spec.timestep_limit, max_traj_len) if max_traj_len is not None else mdp.env_spec.timestep_limit\n",
    "\n",
    "    print 'Sampling {} trajs (max len {}) from policy {} in {}'.format(num_trajs, max_traj_len, policystr, env_name)\n",
    "\n",
    "    # Sample trajs\n",
    "    trajbatch = mdp.sim_mp(\n",
    "        policy_fn=lambda obs_B_Do: policy.sample_actions(obs_B_Do, deterministic),\n",
    "        obsfeat_fn=lambda obs:obs,\n",
    "        cfg=policyopt.SimConfig(\n",
    "            min_num_trajs=num_trajs,\n",
    "            min_total_sa=-1,\n",
    "            batch_size=None,\n",
    "            max_traj_len=max_traj_len))\n",
    "\n",
    "    return trajbatch, policy, mdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95m=== Phase 0: Sampling trajs from expert policies ===\u001b[0m\n",
      "\u001b[95mSampling 50 trajectories\u001b[0m\n",
      "Loading policy parameters from /snapshots/iter0000500 in expert_policies/modern/log_Hopper-v0_3.h5\n",
      "Loading environment Hopper-v1\n",
      "Gym version: 0.9.3\n",
      "MDP observation space, action space sizes: 11, 3\n",
      "\n",
      "\u001b[95mLoading feedforward net specification\u001b[0m\n",
      "[\n",
      "  {\n",
      "    \"type\": \"fc\",\n",
      "    \"n\": 50\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"nonlin\",\n",
      "    \"func\": \"lrelu\"\n",
      "  }\n",
      "]\n",
      "\u001b[95mAffine(in=11, out=50)\u001b[0m\n",
      "\u001b[95mNonlinearity(func=lrelu)\u001b[0m\n",
      "\u001b[95mAffine(in=50, out=3)\u001b[0m\n",
      "Reading GaussianPolicy/logstdevs_1_Da\n",
      "Reading GaussianPolicy/obsnorm/Standardizer/count\n",
      "Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D\n",
      "Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D\n",
      "Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W\n",
      "Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b\n",
      "Reading GaussianPolicy/out/AffineLayer/W\n",
      "Reading GaussianPolicy/out/AffineLayer/b\n",
      "Sampling 50 trajs (max len 1000) from policy expert_policies/modern/log_Hopper-v0_3.h5/snapshots/iter0000500 in Hopper-v1\n",
      "ret: 3598.81385561 +/- 14.0050559377\n",
      "avgr: 3.59881385561\n",
      "len: 1000.0 +/- 0.0\n",
      "ent: 0.77428123166\n",
      "\u001b[95mWrote imitation_runs/test/trajs/trajs_hopper.h5\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#with open('./pipelines/im_classic_pipeline_kj.yaml', 'r') as f:\n",
    "#with open('./pipelines/im_classic_pipeline_sparse.yaml', 'r') as f:\n",
    "# with open('./pipelines/im_test_pipeline.yaml','r') as f:\n",
    "with open('./pipelines/im_test_gail_pipeline.yaml','r') as f:\n",
    "    spec = yaml.load(f)\n",
    "\n",
    "util.header('=== Phase 0: Sampling trajs from expert policies ===')\n",
    "\n",
    "num_trajs = spec['training']['full_dataset_num_trajs']\n",
    "util.header('Sampling {} trajectories'.format(num_trajs))\n",
    "\n",
    "# Make filenames and check if they're valid first\n",
    "taskname2outfile = gen_taskname2outfile(spec, assert_not_exists=True)\n",
    "\n",
    "# Sample trajs for each task\n",
    "for task in spec['tasks']:\n",
    "    # Execute the policy\n",
    "    trajbatch, policy, _ = exec_saved_policy(\n",
    "        task['env'], task['policy'], num_trajs,\n",
    "        deterministic=spec['training']['deterministic_expert'],\n",
    "        max_traj_len=None)\n",
    "\n",
    "    # Quick evaluation\n",
    "    returns = trajbatch.r.padded(fill=0.).sum(axis=1)\n",
    "    avgr = trajbatch.r.stacked.mean()\n",
    "    lengths = np.array([len(traj) for traj in trajbatch])\n",
    "    ent = policy._compute_actiondist_entropy(trajbatch.adist.stacked).mean()\n",
    "    print 'ret: {} +/- {}'.format(returns.mean(), returns.std())\n",
    "    print 'avgr: {}'.format(avgr)\n",
    "    print 'len: {} +/- {}'.format(lengths.mean(), lengths.std())\n",
    "    print 'ent: {}'.format(ent)\n",
    "\n",
    "    # Save the trajs to a file\n",
    "    with h5py.File(taskname2outfile[task['name']], 'w') as f:\n",
    "        def write(dsetname, a):\n",
    "            f.create_dataset(dsetname, data=a, compression='gzip', compression_opts=9)\n",
    "        # Right-padded trajectory data\n",
    "        write('obs_B_T_Do', trajbatch.obs.padded(fill=0.))\n",
    "        write('a_B_T_Da', trajbatch.a.padded(fill=0.))\n",
    "        write('r_B_T', trajbatch.r.padded(fill=0.))\n",
    "        # Trajectory lengths\n",
    "        write('len_B', np.array([len(traj) for traj in trajbatch], dtype=np.int32))\n",
    "        # # Also save args to this script\n",
    "        # argstr = json.dumps(vars(args), separators=(',', ':'), indent=2)\n",
    "        # f.attrs['args'] = argstr\n",
    "    util.header('Wrote {}'.format(taskname2outfile[task['name']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
